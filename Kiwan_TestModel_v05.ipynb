{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hierarchisches Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! is used to run console commands in jupyter notebooks\n",
    "!pip install -q nbstripout\n",
    "!pip install torch-summary\n",
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './KaggleCache/datasets/andrewmvd/ocular-disease-recognition-odir5k/versions/2'\n",
    "df = pd.read_csv(os.path.join(path, 'full_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND PREPROCESSING ---\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# --- HELPER: CONVERT TARGET TO CLASS INDEX ---\n",
    "def target_string_to_index(target_str: str) -> int:\n",
    "    \"\"\"Converts '[0,1,0...]' into integer index.\"\"\"\n",
    "    target_list = ast.literal_eval(target_str)\n",
    "    return target_list.index(1)\n",
    "\n",
    "# 1. Apply conversion to original DF\n",
    "df['class_index'] = df['target'].apply(target_string_to_index)\n",
    "\n",
    "print(\"--- Initial Class Distribution (Original) ---\")\n",
    "print(df['class_index'].value_counts().sort_index())\n",
    "print(\"-\" * 34)\n",
    "\n",
    "\n",
    "# --- 2. PREPARE STAGE 1: BINARY (Healthy vs. Sick) ---\n",
    "print(\"\\n=== STAGE 1 PREPARATION: BINARY ===\")\n",
    "df_binary = df.copy()\n",
    "# Map: 0 remains 0, 1-7 become 1\n",
    "df_binary['class_index'] = df_binary['class_index'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Split Stage 1\n",
    "train_df_binary, temp_df_binary = train_test_split(\n",
    "    df_binary, test_size=(VAL_RATIO + TEST_RATIO), \n",
    "    stratify=df_binary['class_index'], random_state=42\n",
    ")\n",
    "val_df_binary, test_df_binary = train_test_split(\n",
    "    temp_df_binary, test_size=0.5, \n",
    "    stratify=temp_df_binary['class_index'], random_state=42\n",
    ")\n",
    "\n",
    "# Calculate Weights for Stage 1 (Binary)\n",
    "# Simple inverse ratio is enough here\n",
    "count_0 = len(train_df_binary[train_df_binary['class_index'] == 0])\n",
    "count_1 = len(train_df_binary[train_df_binary['class_index'] == 1])\n",
    "# Gewicht für Klasse 0 und 1\n",
    "weights_binary_np = np.array([1.0, count_0 / count_1], dtype=np.float32)\n",
    "print(f\"Binary Training Sizes: 0 (Normal)={count_0}, 1 (Sick)={count_1}\")\n",
    "print(f\"Binary Weights: {weights_binary_np}\")\n",
    "\n",
    "\n",
    "# --- 3. PREPARE STAGE 2: SPECIALIST (Disease Only) ---\n",
    "print(\"\\n=== STAGE 2 PREPARATION: SPECIALIST ===\")\n",
    "# Filter: Keep only sick\n",
    "df_disease = df[df['class_index'] != 0].copy()\n",
    "\n",
    "# Shift Labels: 1->0, 2->1 ... 7->6\n",
    "df_disease['class_index'] = df_disease['class_index'] - 1\n",
    "\n",
    "# Split Stage 2\n",
    "train_df_dis, temp_df_dis = train_test_split(\n",
    "    df_disease, test_size=(VAL_RATIO + TEST_RATIO), \n",
    "    stratify=df_disease['class_index'], random_state=42\n",
    ")\n",
    "val_df_dis, test_df_dis = train_test_split(\n",
    "    temp_df_dis, test_size=0.5, \n",
    "    stratify=temp_df_dis['class_index'], random_state=42\n",
    ")\n",
    "\n",
    "# Calculate Weights for Stage 2 (Inverse Frequency) - Dein alter Code, angepasst\n",
    "class_counts = Counter(train_df_dis['class_index'])\n",
    "total_samples = len(train_df_dis)\n",
    "NUM_CLASSES_DIS = 7\n",
    "\n",
    "class_frequencies = {i: class_counts.get(i, 0) / total_samples for i in range(NUM_CLASSES_DIS)}\n",
    "class_weights = {i: 1.0 / class_frequencies[i] for i in range(NUM_CLASSES_DIS) if class_frequencies[i] > 0}\n",
    "inverse_weights = [class_weights.get(i, 0.0) for i in range(NUM_CLASSES_DIS)]\n",
    "\n",
    "# Normalize\n",
    "max_weight = max(inverse_weights)\n",
    "weights_specialist_np = np.array([w / max_weight for w in inverse_weights], dtype=np.float32)\n",
    "\n",
    "print(\"Specialist Class Weights (Normalized):\")\n",
    "print(weights_specialist_np)\n",
    "# Index 4 ist jetzt Hypertension, Index 5 ist Myopia (weil alles eins gerutscht ist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## ResNet18 and ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL DEFINITIONS ---\n",
    "\n",
    "class ResNet18WithSideInfo(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(ResNet18WithSideInfo, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.num_ftrs = self.resnet.fc.in_features \n",
    "        self.resnet.fc = nn.Identity() \n",
    "        self.final_fc = nn.Linear(self.num_ftrs + 2, num_classes)\n",
    "\n",
    "    def forward(self, image, side_vector):\n",
    "        features = self.resnet(image)\n",
    "        combined = torch.cat((features, side_vector), dim=1)\n",
    "        output = self.final_fc(combined)\n",
    "        return output\n",
    "\n",
    "class ResNet50WithSideInfo(nn.Module):\n",
    "    \"\"\"\n",
    "    Der große Bruder: Mehr Parameter (23M vs 11M), tieferes Netzwerk.\n",
    "    Ideal für Stage 1 (Binary), da wir hier viele Daten haben.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet50WithSideInfo, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # ResNet50 hat 2048 Features am Ende (ResNet18 hat nur 512)\n",
    "        self.num_ftrs = self.resnet.fc.in_features \n",
    "        \n",
    "        self.resnet.fc = nn.Identity() \n",
    "        self.final_fc = nn.Linear(self.num_ftrs + 2, num_classes)\n",
    "\n",
    "    def forward(self, image, side_vector):\n",
    "        features = self.resnet(image)\n",
    "        combined = torch.cat((features, side_vector), dim=1)\n",
    "        output = self.final_fc(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Pipeline: Custom Dataset & Mirroring Strategy\n",
    "\n",
    "# --- TRANSFORMS ---\n",
    "# Standard ImageNet normalization stats\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training Transforms (High-Res 512x512)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    # NOTE: No RandomHorizontalFlip here! \n",
    "    # Mirroring is handled logically in the Dataset class to align optic disc position.\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Validation/Test Transforms (Deterministic)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# --- CUSTOM DATASET CLASS ---\n",
    "class OcularDatasetSideAware(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset that handles image loading and side-specific preprocessing.\n",
    "    Implements the 'Mirroring Trick': Right eyes are flipped to structurally resemble left eyes.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(self.root_dir, \"preprocessed_images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['filename']\n",
    "        label = row['class_index']\n",
    "        \n",
    "        # 1. Side Detection & Encoding\n",
    "        is_right_eye = 'right' in img_name\n",
    "        # One-Hot Encoding: [1, 0] for Left, [0, 1] for Right\n",
    "        side_vector = torch.tensor([0.0, 1.0]) if is_right_eye else torch.tensor([1.0, 0.0])\n",
    "\n",
    "        # 2. Load Image\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 3. Apply Mirroring Trick\n",
    "        # Flip right eyes horizontally so optic disc is always on the same side (nasal)\n",
    "        if is_right_eye:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        # 4. Apply Transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, side_vector, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Hierarchical Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING CONFIGURATION & EXECUTION (HIERARCHICAL) ---\n",
    "\n",
    "# --- GLOBAL SETTINGS ---\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def train_one_epoch_side(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, sides, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images, sides, labels = images.to(device), sides.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, sides)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate_epoch_side(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, sides, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images, sides, labels = images.to(device), sides.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, sides)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return running_loss / len(dataloader.dataset), accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 1: BINARY TRAINING (Healthy vs. Sick)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n>>> STARTING STAGE 1: BINARY MODEL <<<\\n\" + \"=\"*40)\n",
    "\n",
    "# 1. Config Stage 1\n",
    "NUM_EPOCHS_BINARY = 15\n",
    "LR_BINARY = 1e-4\n",
    "\n",
    "count_0 = len(train_df_binary[train_df_binary['class_index'] == 0])\n",
    "count_1 = len(train_df_binary[train_df_binary['class_index'] == 1])\n",
    "weights_binary = [1.0, count_0 / count_1] \n",
    "sample_weights_binary = train_df_binary['class_index'].apply(lambda x: weights_binary[x]).values\n",
    "sampler_binary = WeightedRandomSampler(weights=sample_weights_binary, num_samples=len(sample_weights_binary), replacement=True)\n",
    "\n",
    "# 2. Loaders\n",
    "train_ds_binary = OcularDatasetSideAware(train_df_binary, path, transform=train_transforms)\n",
    "val_ds_binary = OcularDatasetSideAware(val_df_binary, path, transform=val_test_transforms)\n",
    "train_loader_binary = DataLoader(train_ds_binary, batch_size=16, sampler=sampler_binary, num_workers=4)\n",
    "val_loader_binary = DataLoader(val_ds_binary, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# 3. Model Setup\n",
    "model_binary = ResNet50WithSideInfo(num_classes=2).to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model_binary.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "\n",
    "# Training Loop\n",
    "best_f1 = 0.0\n",
    "for epoch in range(1, 16): \n",
    "    model_binary.train()\n",
    "    for img, side, lbl in tqdm(train_loader_binary, desc=f\"Ep {epoch} Binary Train (R50)\", leave=False):\n",
    "        img, side, lbl = img.to(DEVICE), side.to(DEVICE), lbl.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_binary(img, side), lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model_binary.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, side, lbl in val_loader_binary:\n",
    "            img, side, lbl = img.to(DEVICE), side.to(DEVICE), lbl.to(DEVICE)\n",
    "            out = model_binary(img, side)\n",
    "            preds.extend(torch.argmax(out, 1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "            \n",
    "    val_f1 = f1_score(targets, preds, average='weighted', zero_division=0)\n",
    "    print(f\"Binary (R50) Epoch {epoch}: Val F1 {val_f1:.4f}\")\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model_binary.state_dict(), 'best_binary_model_r50.pth')\n",
    "        print(\"  --> Best Binary Model saved!\")\n",
    "\n",
    "print(\"Stage 1 Done. Saved 'best_binary_model_r50.pth'\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 2: SPECIALIST TRAINING (7 Diseases)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n>>> STARTING STAGE 2: SPECIALIST MODEL <<<\\n\" + \"=\"*40)\n",
    "\n",
    "# 1. Config Stage 2\n",
    "NUM_EPOCHS_DIS = 30\n",
    "LR_DIS = 1e-4\n",
    "\n",
    "# 2. Weights & Sampler Stage 2\n",
    "# Berechnung der Basis-Gewichte\n",
    "class_counts = Counter(train_df_dis['class_index'])\n",
    "weights_dis_raw = {i: 1.0/class_counts[i] for i in range(7)}\n",
    "max_w = max(weights_dis_raw.values())\n",
    "weights_dis_norm = [weights_dis_raw[i]/max_w for i in range(7)]\n",
    "\n",
    "# Sampler erstellen\n",
    "sample_weights_dis = train_df_dis['class_index'].apply(lambda x: weights_dis_norm[x]).values\n",
    "sampler_dis = WeightedRandomSampler(weights=sample_weights_dis, num_samples=len(sample_weights_dis), replacement=True)\n",
    "\n",
    "# Loaders\n",
    "train_loader_dis = DataLoader(OcularDatasetSideAware(train_df_dis, path, transform=train_transforms), batch_size=BATCH_SIZE, sampler=sampler_dis, num_workers=4)\n",
    "val_loader_dis = DataLoader(OcularDatasetSideAware(val_df_dis, path, transform=val_test_transforms), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# 3. Model Setup Stage 2 & LOSS GEWICHTE\n",
    "model_dis = ResNet18WithSideInfo(num_classes=7).to(DEVICE)\n",
    "optimizer = optim.AdamW(model_dis.parameters(), lr=LR_DIS, weight_decay=1e-5)\n",
    "\n",
    "# HOLZHAMMER: Wir manipulieren die Loss-Gewichte manuell\n",
    "loss_weights = np.array(weights_dis_norm, dtype=np.float32)\n",
    "# Index 4 = Hypertension (verschoben von 5) -> x5 Boost\n",
    "loss_weights[4] *= 5.0 \n",
    "# Index 5 = Myopia (verschoben von 6) -> x3 Boost\n",
    "loss_weights[5] *= 3.0\n",
    "print(\"Specialist Loss Weights (inkl. Boost):\", loss_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(loss_weights).to(DEVICE)).to(DEVICE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
    "\n",
    "# 4. Main Loop Stage 2\n",
    "history_dis = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS_DIS + 1):\n",
    "    train_loss = train_one_epoch_side(model_dis, train_loader_dis, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, val_f1 = validate_epoch_side(model_dis, val_loader_dis, criterion, DEVICE)\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    history_dis['train_loss'].append(train_loss)\n",
    "    history_dis['val_loss'].append(val_loss)\n",
    "    history_dis['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"Specialist Epoch {epoch}: Train Loss {train_loss:.4f} | Val F1 {val_f1:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model_dis.state_dict(), 'best_specialist_model_r18.pth')\n",
    "        print(\"  --> Best Specialist Model saved!\")\n",
    "\n",
    "print(f\"Stage 2 Done. Best F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. EVALUATION & VISUALIZATION (HIERARCHICAL) ---\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# WICHTIG: Korrekte Reihenfolge nach ODIR-Standard\n",
    "CLASS_NAMES = [\n",
    "    \"Normal\",           # 0\n",
    "    \"Diabetes\",         # 1\n",
    "    \"Glaucoma\",         # 2\n",
    "    \"Cataract\",         # 3\n",
    "    \"Macular Deg.\",     # 4\n",
    "    \"Hypertension\",     # 5\n",
    "    \"Myopia\",           # 6\n",
    "    \"Other\"             # 7\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 2. PLOTTING HELPERS ---\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    \"\"\"Plots a seaborn heatmap of the confusion matrix.\"\"\"\n",
    "    # Ensure labels match the length of class_names\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=np.arange(len(class_names)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title('Hierarchical Model Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. HIERARCHICAL PREDICTION LOGIC ---\n",
    "def predict_hierarchical(binary_model, specialist_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Kombiniert Stage 1 (Binary) und Stage 2 (Specialist).\n",
    "    \"\"\"\n",
    "    binary_model.eval()\n",
    "    specialist_model.eval()\n",
    "    \n",
    "    final_preds = []\n",
    "    final_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, sides, labels in tqdm(dataloader, desc=\"Hierarchical Inference\"):\n",
    "            images, sides = images.to(device), sides.to(device)\n",
    "            \n",
    "            # A: Stage 1 - Frag den Türsteher (Binary Model)\n",
    "            binary_out = binary_model(images, sides)\n",
    "            is_sick_preds = torch.argmax(binary_out, dim=1).cpu().numpy() # 0=Gesund, 1=Krank\n",
    "            \n",
    "            # B: Stage 2 - Frag den Spezialisten (Specialist Model)\n",
    "            # Wir lassen ihn ALLE Bilder bewerten (technisch einfacher als rauspicken)\n",
    "            spec_out = specialist_model(images, sides)\n",
    "            disease_preds = torch.argmax(spec_out, dim=1).cpu().numpy() # 0-6 (Krankheiten)\n",
    "            \n",
    "            # C: Entscheidungs-Logik kombinieren\n",
    "            batch_results = []\n",
    "            for i in range(len(labels)):\n",
    "                if is_sick_preds[i] == 0:\n",
    "                    # Stage 1 sagt: Gesund\n",
    "                    batch_results.append(0) # Label 0 = Normal\n",
    "                else:\n",
    "                    # Stage 1 sagt: Krank -> Wir vertrauen Stage 2\n",
    "                    # Stage 2 Output (0-6) muss zurückgerechnet werden auf (1-7)\n",
    "                    # Bsp: Stage 2 sagt 0 (Diabetes) -> Global Label 1 (Diabetes)\n",
    "                    # Bsp: Stage 2 sagt 4 (Hypertension) -> Global Label 5 (Hypertension)\n",
    "                    global_label = disease_preds[i] + 1\n",
    "                    batch_results.append(global_label)\n",
    "            \n",
    "            final_preds.extend(batch_results)\n",
    "            final_labels.extend(labels.numpy())\n",
    "            \n",
    "    return np.array(final_labels), np.array(final_preds)\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\nSTARTING FINAL EVALUATION\\n\" + \"=\"*40)\n",
    "\n",
    "# A. Load Test Data (Original, unsplit Labels 0-7)\n",
    "# Wir nutzen test_df_binary als Basis für die Indizes, laden aber die originalen Labels aus df\n",
    "print(\"Loading Original Test Data...\")\n",
    "test_indices = test_df_binary.index\n",
    "df_test_final = df.loc[test_indices].copy() \n",
    "\n",
    "# Dataset & Loader erstellen\n",
    "test_ds_final = OcularDatasetSideAware(df_test_final, path, transform=val_test_transforms)\n",
    "test_loader_final = DataLoader(test_ds_final, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# 1. Load Models\n",
    "# Stage 1: ResNet50 (Gesund vs. krank)\n",
    "model_binary = ResNet50WithSideInfo(num_classes=2).to(DEVICE)\n",
    "model_binary.load_state_dict(torch.load('best_binary_model_r50.pth', weights_only=True))\n",
    "\n",
    "# Stage 2: ResNet18 (Krankheits-Klassen)\n",
    "model_dis = ResNet18WithSideInfo(num_classes=7).to(DEVICE)\n",
    "model_dis.load_state_dict(torch.load('best_specialist_model_r18.pth', weights_only=True))\n",
    "\n",
    "# 2. TTA Prediction Function\n",
    "def predict_hierarchical_tta(loader):\n",
    "    model_binary.eval(); model_dis.eval()\n",
    "    final_preds, final_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, side, lbl in tqdm(loader, desc=\"Inference with TTA\"):\n",
    "            img, side = img.to(DEVICE), side.to(DEVICE)\n",
    "            \n",
    "            # --- TTA LOGIC START ---\n",
    "            \n",
    "            # Variante 1: Original\n",
    "            p_binary_1 = F.softmax(model_binary(img, side), dim=1)\n",
    "            p_spec_1 = F.softmax(model_dis(img, side), dim=1)\n",
    "            \n",
    "            # Variante 2: Rotation +5 Grad\n",
    "            img_rot_pos = TF.rotate(img, 5)\n",
    "            p_binary_2 = F.softmax(model_binary(img_rot_pos, side), dim=1)\n",
    "            p_spec_2 = F.softmax(model_dis(img_rot_pos, side), dim=1)\n",
    "\n",
    "            # Variante 3: Rotation -5 Grad\n",
    "            img_rot_neg = TF.rotate(img, -5)\n",
    "            p_binary_3 = F.softmax(model_binary(img_rot_neg, side), dim=1)\n",
    "            p_spec_3 = F.softmax(model_dis(img_rot_neg, side), dim=1)\n",
    "            \n",
    "            # DURCHSCHNITT BILDEN\n",
    "            avg_binary = (p_binary_1 + p_binary_2 + p_binary_3) / 3.0\n",
    "            avg_spec = (p_spec_1 + p_spec_2 + p_spec_3) / 3.0\n",
    "            # --- TTA LOGIC END ---\n",
    "\n",
    "            # Entscheidungen treffen (basierend auf Durchschnitt)\n",
    "            is_sick = torch.argmax(avg_binary, 1).cpu().numpy() # 0/1\n",
    "            spec_out = torch.argmax(avg_spec, 1).cpu().numpy() # 0-6\n",
    "            \n",
    "            # Kombinieren\n",
    "            batch_preds = []\n",
    "            for i in range(len(is_sick)):\n",
    "                if is_sick[i] == 0:\n",
    "                    batch_preds.append(0) # Normal\n",
    "                else:\n",
    "                    batch_preds.append(spec_out[i] + 1) # Shift +1\n",
    "            \n",
    "            final_preds.extend(batch_preds)\n",
    "            final_labels.extend(lbl.numpy())\n",
    "            \n",
    "    return np.array(final_labels), np.array(final_preds)\n",
    "\n",
    "# Run\n",
    "y_true, y_pred = predict_hierarchical_tta(test_loader_final)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Final TTA Accuracy: {acc:.4f}\")\n",
    "print(f\"Final TTA F1: {f1:.4f}\")\n",
    "\n",
    "CLASS_NAMES = [\"Normal\", \"Diabetes\", \"Glaucoma\", \"Cataract\", \"Macular Deg.\", \"Hypertension\", \"Myopia\", \"Other\"]\n",
    "plot_confusion_matrix(y_true, y_pred, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wrapper, damit GradCAM mit den 2 Inputs (Bild + Seite) klarkommt\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, side_vector):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.side = side_vector.unsqueeze(0).to(DEVICE) # Batch dimension hinzufügen\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, self.side)\n",
    "\n",
    "def visualize_gradcam(model, dataset, num_images):\n",
    "    model.eval()\n",
    "    \n",
    "    # Letzter Convolutional Layer\n",
    "    target_layers = [model.resnet.layer4[-1]]\n",
    "    \n",
    "    indices = np.where(df_test_final['class_index'] > 0)[0] # Nur Kranke\n",
    "    selected_indices = np.random.choice(indices, num_images, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*num_images))\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        img_tensor, side, label_idx = dataset[idx]\n",
    "        \n",
    "        # Originalbild für Anzeige (denormalisieren)\n",
    "        inv_normalize = transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "        img_display = inv_normalize(img_tensor).permute(1, 2, 0).numpy()\n",
    "        img_display = np.clip(img_display, 0, 1) # Sicherstellen, dass Pixel zwischen 0-1 sind\n",
    "        \n",
    "        # Wrapper erstellen (fixiert die Seite für dieses eine Bild)\n",
    "        wrapped_model = ModelWrapper(model, side)\n",
    "        \n",
    "        # GradCAM initialisieren\n",
    "        cam = GradCAM(model=wrapped_model, target_layers=target_layers)\n",
    "        \n",
    "        # Wir fragen: \"Warum denkst du, ist das Klasse 1 (Krank)?\"\n",
    "        targets = [ClassifierOutputTarget(1)]\n",
    "        \n",
    "        # Heatmap generieren\n",
    "        input_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        \n",
    "        # Overlay erstellen\n",
    "        visualization = show_cam_on_image(img_display, grayscale_cam, use_rgb=True)\n",
    "        \n",
    "        # Plotting\n",
    "        true_label = CLASS_NAMES[label_idx]\n",
    "        \n",
    "        # Original\n",
    "        ax = plt.subplot(num_images, 2, 2*i + 1)\n",
    "        plt.imshow(img_display)\n",
    "        plt.title(f\"Original: {true_label}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # GradCAM\n",
    "        ax = plt.subplot(num_images, 2, 2*i + 2)\n",
    "        plt.imshow(visualization)\n",
    "        plt.title(f\"GradCAM (Focus for 'Sick')\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisierung von Stage 1 Modell (ResNet50), da dieses das Kranksein feststellt\n",
    "print(\"Visualizing Stage 1 (ResNet50) Attention...\")\n",
    "visualize_gradcam(model_bin, test_ds_final, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
