{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Training with resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! is used to run console commands in jupyter notebooks\n",
    "!pip install -q nbstripout\n",
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './KaggleCache/datasets/andrewmvd/ocular-disease-recognition-odir5k/versions/2'\n",
    "df = pd.read_csv(os.path.join(path, 'full_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND DATA ASSUMPTION ---\n",
    "\n",
    "# Define the split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "# The number of unique classes\n",
    "NUM_CLASSES = 8\n",
    "\n",
    "# --- 2. DATA PREPROCESSING: CONVERT TARGET TO CLASS INDEX ---\n",
    "\n",
    "def target_string_to_index(target_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Converts a string representation of a one-hot list (e.g., '[0,1,0,...]')\n",
    "    into a single integer class index (e.g., 1).\n",
    "    \"\"\"\n",
    "    # Use ast.literal_eval for safe string-to-list conversion\n",
    "    target_list = ast.literal_eval(target_str)\n",
    "    # The index of '1' is the class index\n",
    "    return target_list.index(1)\n",
    "\n",
    "# Apply the conversion to create the necessary column for stratification\n",
    "df['class_index'] = df['target'].apply(target_string_to_index)\n",
    "\n",
    "# Print initial class distribution\n",
    "print(\"--- Initial Class Distribution ---\")\n",
    "print(df['class_index'].value_counts().sort_index())\n",
    "print(\"-\" * 34)\n",
    "\n",
    "# --- 3. STRATIFIED TRAIN / TEST / VAL SPLIT (70/15/15) ---\n",
    "\n",
    "# Step 1: Split into Training (70%) and Temporary (30%) sets\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(VAL_RATIO + TEST_RATIO), # 0.15 + 0.15 = 0.30\n",
    "    stratify=df['class_index'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split Temporary (30%) into Validation (15%) and Test (15%) sets\n",
    "# test_size = 0.5 because 0.5 of the remaining 0.30 is 0.15\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['class_index'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Verify the final split sizes\n",
    "print(\"\\n--- Final Dataset Sizes ---\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Training Samples (70%): {len(train_df)}\")\n",
    "print(f\"Validation Samples (15%): {len(val_df)}\")\n",
    "print(f\"Test Samples (15%): {len(test_df)}\")\n",
    "\n",
    "# --- 4. CALCULATE INVERSE CLASS FREQUENCY FOR WEIGHTED SAMPLER ---\n",
    "\n",
    "# Count occurrences of each class in the training set\n",
    "class_counts = Counter(train_df['class_index'])\n",
    "# Get total number of samples in the training set\n",
    "total_samples = len(train_df)\n",
    "# Calculate the frequency of each class\n",
    "class_frequencies = {i: class_counts.get(i, 0) / total_samples for i in range(NUM_CLASSES)}\n",
    "\n",
    "# Calculate inverse frequency (or weight)\n",
    "# The weight for a class is inversely proportional to its frequency: w_i = 1 / f_i\n",
    "# We use this as the basis for the PyTorch WeightedRandomSampler\n",
    "class_weights = {\n",
    "    i: 1.0 / class_frequencies[i]\n",
    "    for i in range(NUM_CLASSES) if class_frequencies[i] > 0\n",
    "}\n",
    "\n",
    "# Convert weights to a tensor (PyTorch requires this format)\n",
    "# Note: PyTorch expects weights ordered by class index [w0, w1, w2, ...]\n",
    "# Use max(class_weights.values()) for normalization, but absolute inverse frequency is fine too\n",
    "inverse_weights = [class_weights.get(i, 0.0) for i in range(NUM_CLASSES)]\n",
    "# Normalize the weights so the smallest weight is 1.0\n",
    "max_weight = max(inverse_weights)\n",
    "normalized_weights = [w / max_weight for w in inverse_weights]\n",
    "\n",
    "# Print the final class weights for review\n",
    "print(\"\\n--- Training Set Class Weights (Normalized) ---\")\n",
    "print(f\"Class Frequencies: {class_frequencies}\")\n",
    "print(f\"Inverse Weights: {normalized_weights}\")\n",
    "# Store the weights as a numpy array for easy conversion to PyTorch tensor later\n",
    "class_weights_np = np.array(normalized_weights, dtype=np.float32)\n",
    "\n",
    "print(\"\\n--- Stratification Check (Training Set) ---\")\n",
    "print(train_df['class_index'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"\\n--- Stratification Check (Validation Set) ---\")\n",
    "print(val_df['class_index'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"\\n--- Stratification Check (Test Set) ---\")\n",
    "print(test_df['class_index'].value_counts(normalize=True).sort_index() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Train with resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ocular_resnet18(num_classes=8, feature_extract=True):\n",
    "    \"\"\"\n",
    "    Constructs a ResNet18 model pre-trained on ImageNet and adapts the classification head\n",
    "    for the specific ocular disease task.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of output classes (target diseases).\n",
    "        feature_extract (bool): If True, freezes the backbone weights to keep pre-trained knowledge.\n",
    "    \"\"\"\n",
    "    print(\"Initializing Pre-trained ResNet18...\")\n",
    "    \n",
    "    # 1. Load the pre-trained model with default ImageNet weights\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    # 2. Freeze the parameters (weights) of the backbone\n",
    "    # This prevents the pre-trained features from being destroyed during the initial training phase\n",
    "    if feature_extract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 3. Replace the Head (Fully Connected Layer)\n",
    "    # The original ResNet fc layer takes 512 input features.\n",
    "    # We replace it with a new layer that maps to our num_classes (8).\n",
    "    # Note: New layers automatically have requires_grad=True.\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- MODEL INSTANTIATION ---\n",
    "\n",
    "# Initialize the model\n",
    "ocular_model = build_ocular_resnet18(num_classes=NUM_CLASSES, feature_extract=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ocular_model.to(device)\n",
    "\n",
    "# Sanity Check: Verify that only the last layer is trainable\n",
    "print(\"\\n--- Parameter Check ---\")\n",
    "total_params = sum(p.numel() for p in ocular_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in ocular_model.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters (Head only): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Batch size is a critical hyperparameter; 32 is a common starting point for 512x512\n",
    "BATCH_SIZE = 32\n",
    "# Assumed to be available from previous cells:\n",
    "# train_df, val_df, test_df (DataFrames for splits)\n",
    "# path (Root path string)\n",
    "# class_weights_np (Numpy array of class weights for WeightedRandomSampler)\n",
    "\n",
    "# --- 2. CUSTOM DATASET CLASS (The PyTorch Way) ---\n",
    "\n",
    "class OcularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading ocular fundus images and labels.\n",
    "    Handles image loading, preprocessing, and string-to-index target parsing.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing 'filename' and 'class_index'.\n",
    "            root_dir: Base directory containing the 'preprocessed_images' folder.\n",
    "            transform: Composed torchvision transforms to apply to the image.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(self.root_dir, \"preprocessed_images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads one sample (image and label) based on index.\n",
    "        \"\"\"\n",
    "        # Get filename and class index from the DataFrame\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['filename']\n",
    "        # Target is an integer class index (0-7), converted to LongTensor for CrossEntropyLoss\n",
    "        label = row['class_index'] \n",
    "        \n",
    "        # Construct the full image path\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load the image using PIL (standard for torchvision transforms)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the tensor and the label\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- 3. TRANSFORMS DEFINITION ---\n",
    "\n",
    "# Standard normalization stats for ImageNet-trained models\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    # Resize to 224x224 (Standard for ResNet) to improve performance and speed\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Augmentations for robustness\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    # Convert to Tensor and Normalize\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Validation/Test Transforms (Deterministic)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# --- 4. DATASET INSTANTIATION ---\n",
    "\n",
    "train_dataset = OcularDataset(train_df, path, transform=train_transforms)\n",
    "val_dataset = OcularDataset(val_df, path, transform=val_test_transforms)\n",
    "test_dataset = OcularDataset(test_df, path, transform=val_test_transforms)\n",
    "\n",
    "# --- 5. DATA LOADER CONFIGURATION (Addressing Imbalance) ---\n",
    "\n",
    "# Calculate sample weights for the Training DataLoader\n",
    "# Map the class index of every sample in the training set to its inverse weight\n",
    "sample_weights = train_df['class_index'].apply(lambda x: class_weights_np[x]).values\n",
    "# Create the sampler: replaces the standard shuffling behavior\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights), # Sample size equals the full dataset length\n",
    "    replacement=True # Must be True for random selection with replacement\n",
    ")\n",
    "\n",
    "# Instantiate the DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler, # Use the custom sampler instead of 'shuffle=True'\n",
    "    num_workers=4, # Use multiple threads for faster data loading (best practice)\n",
    "    pin_memory=True # Speeds up data transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No shuffling needed for validation\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No shuffling needed for testing\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- 6. SANITY CHECK ---\n",
    "\n",
    "print(f\"\\n--- DataLoader Sanity Check ---\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check one batch to verify tensor shapes and device readiness\n",
    "for images, labels in train_loader:\n",
    "    print(f\"\\nSample Batch Test (Training Loader):\")\n",
    "    print(f\"Image Tensor Shape (B x C x H x W): {images.shape}\")\n",
    "    print(f\"Label Tensor Shape (B): {labels.shape}\")\n",
    "    print(f\"Label Data Type (Should be torch.long): {labels.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Training (resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Hyperparameters (initial estimates)\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "# REGULARIZATION FIXES for Overfitting and high complexity\n",
    "# PATIENCE removed to disable Early Stopping, allowing full run\n",
    "WEIGHT_DECAY = 1e-5    # L2 regularization to penalize large weights and reduce overfitting\n",
    "\n",
    "# Assumed to be available from previous cells:\n",
    "# ocular_model (Instance of OcularCNN, moved to 'device')\n",
    "# train_loader, val_loader (DataLoaders)\n",
    "# device (torch.device('cuda:0' or 'cpu'))\n",
    "# NUM_CLASSES (int, 8)\n",
    "\n",
    "# --- 2. INITIALIZATION ---\n",
    "\n",
    "# Define the optimizer\n",
    "# KEY CHANGE: We filter the parameters to only pass those with requires_grad=True (the new head).\n",
    "# This prevents the optimizer from trying to update the frozen backbone weights.\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ocular_model.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "# --- 3. TRAINING FUNCTION DEFINITION ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Executes a single training epoch with explicit forward and backward passes.\n",
    "    \"\"\"\n",
    "    model.train() # Set the model to training mode (enables dropout/batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap the loader with tqdm for a progress bar\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 1. Device Transfer: Move data to the active device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 2. Optimization Step 1: Zero the gradients\n",
    "        # Crucial in PyTorch to prevent gradient accumulation from previous batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 3. Forward Pass: Compute model output (logits)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 4. Loss Calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 5. Optimization Step 2: Backward Pass (Backpropagation)\n",
    "        # Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # 6. Optimization Step 3: Update Weights\n",
    "        # Optimizer steps, adjusting parameters based on calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# --- 4. VALIDATION FUNCTION DEFINITION ---\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set without updating weights.\n",
    "    Also collects raw predictions and labels for confusion matrices.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode (disables dropout/batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient calculations during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            # Device Transfer\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Convert logits to predicted class indices\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and true labels for metric calculation on CPU\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # F1-score is important for imbalanced data; 'weighted' accounts for imbalance\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0) \n",
    "    \n",
    "    # Return metrics PLUS raw arrays needed for confusion matrix\n",
    "    return epoch_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# --- 5. MAIN TRAINING LOOP EXECUTION ---\n",
    "\n",
    "print(f\"\\n--- Starting Training on {device} for {NUM_EPOCHS} epochs ---\")\n",
    "\n",
    "# ARCHITECT FIX: Re-assert model device to fix the RuntimeError \n",
    "# This ensures all model parameters are definitely on the GPU before training starts.\n",
    "ocular_model.to(device)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "# Initialize lists to store metrics history for later plotting\n",
    "history_train_loss = []\n",
    "history_val_loss = []\n",
    "history_val_acc = []\n",
    "history_val_f1 = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(ocular_model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate (Note: validation now returns raw predictions and labels too)\n",
    "    val_loss, val_acc, val_f1, _, _ = validate_epoch(ocular_model, val_loader, criterion, device)\n",
    "\n",
    "    # Store history\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_val_loss.append(val_loss)\n",
    "    history_val_acc.append(val_acc)\n",
    "    history_val_f1.append(val_f1)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} | F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Model Checkpointing Logic (Save only if improved)\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        # Save only the model's learned parameters (state_dict)\n",
    "        torch.save(ocular_model.state_dict(), 'best_ocular_cnn.pth')\n",
    "        print(\"  --> Model saved! New best F1-Score achieved.\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation F1-Score: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: FINE-TUNING ---\n",
    "\n",
    "print(\"\\n--- Start Fine-Tuning ---\")\n",
    "\n",
    "# 1. Load the best model from the previous step to ensure we start from the peak\n",
    "# (This prevents starting from the overfitting/oscillating state of epoch 20)\n",
    "ocular_model.load_state_dict(torch.load('best_ocular_cnn.pth', weights_only=True))\n",
    "\n",
    "# 2. Unfreeze ALL layers\n",
    "# Now we allow the updates to propagate back into the ResNet backbone\n",
    "for param in ocular_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. Re-initialize Optimizer with a MUCH LOWER learning rate\n",
    "# High LR would destroy the pre-trained features. We use 1e-5 (100x smaller).\n",
    "FINE_TUNE_LR = 1e-5\n",
    "optimizer = optim.AdamW(ocular_model.parameters(), lr=FINE_TUNE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# 4. Train for a few more epochs (e.g., 10-15)\n",
    "NUM_FINETUNE_EPOCHS = 15\n",
    "print(f\"Fine-tuning for {NUM_FINETUNE_EPOCHS} epochs with LR={FINE_TUNE_LR}...\")\n",
    "\n",
    "# Reuse the same training loop logic\n",
    "best_val_f1 = 0.4933 # Set current best to beat (from your log)\n",
    "\n",
    "for epoch in range(1, NUM_FINETUNE_EPOCHS + 1):\n",
    "    current_epoch = NUM_EPOCHS + epoch # Just for display (e.g., Epoch 21)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(ocular_model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = validate_epoch(ocular_model, val_loader, criterion, device)\n",
    "\n",
    "    # Append to history (so we can plot everything together later)\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_val_loss.append(val_loss)\n",
    "    history_val_acc.append(val_acc)\n",
    "    history_val_f1.append(val_f1)\n",
    "\n",
    "    print(f\"\\nEpoch {current_epoch}/{NUM_EPOCHS + NUM_FINETUNE_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} | F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save if improved\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(ocular_model.state_dict(), 'best_ocular_cnn_finetuned.pth')\n",
    "        print(\"  --> Model saved! New best F1-Score achieved (Fine-Tuned).\")\n",
    "\n",
    "print(f\"\\n--- Fine-Tuning Complete. Best F1: {best_val_f1:.4f} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Plotting (after Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Assumed to be available from previous cells:\n",
    "# ocular_model (Instance of OcularCNN)\n",
    "# test_loader (DataLoader for the final, unseen data)\n",
    "# device (torch.device('cuda:0' or 'cpu'))\n",
    "# history_train_loss, history_val_loss, history_val_f1, history_val_acc (Metric lists from training)\n",
    "# NUM_CLASSES (int, 8)\n",
    "\n",
    "# Define class names for the confusion matrix display\n",
    "# The order must match the class indices (0 to 7)\n",
    "CLASS_NAMES = [\n",
    "    \"Normal\", \"Diabetic\", \"Glaucoma\", \"Cataract\", \n",
    "    \"Macular Deg.\", \"Retinal Detach.\", \"Hypertensive\", \"Other\"\n",
    "]\n",
    "\n",
    "# --- 2. PLOTTING FUNCTIONS ---\n",
    "\n",
    "def plot_training_metrics(train_loss, val_loss, val_f1, val_acc):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and F1-score/Accuracy over epochs.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Loss Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (CrossEntropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: F1 and Accuracy Scores\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_f1, 'go-', label='Validation F1-Score (Weighted)')\n",
    "    plt.plot(epochs, val_acc, 'yo--', label='Validation Accuracy')\n",
    "    plt.title('Validation Performance')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_test_predictions(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Runs the model on the test set to collect all predictions and true labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "            # Get the predicted class index\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    \"\"\"\n",
    "    Generates and plots the confusion matrix for final classification results.\n",
    "    \"\"\"\n",
    "    # Calculate the raw confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=np.arange(len(class_names)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Use heatmap for visualization with seaborn\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', # 'd' formats the numbers as integers\n",
    "        cmap='Blues', \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "# --- 3. EXECUTION ---\n",
    "\n",
    "# 1. Plot Training History\n",
    "print(\"\\n--- Plotting Training Metrics ---\")\n",
    "plot_training_metrics(\n",
    "    history_train_loss, \n",
    "    history_val_loss, \n",
    "    history_val_f1, \n",
    "    history_val_acc\n",
    ")\n",
    "\n",
    "# 2. Evaluate on Test Set and Plot Confusion Matrix\n",
    "\n",
    "# Load the best weights saved during training\n",
    "# FIX: Added weights_only=True to comply with PyTorch best practice and remove Future Warning\n",
    "#ocular_model.load_state_dict(torch.load('best_ocular_cnn.pth', weights_only=True))\n",
    "# Ändere den Namen auf die _finetuned Version\n",
    "ocular_model.load_state_dict(torch.load('best_ocular_cnn_finetuned.pth', weights_only=True))\n",
    "ocular_model.to(device) # Ensure model is on device before running inference\n",
    "\n",
    "# Get predictions and true labels from the unseen test set\n",
    "test_preds, test_labels = get_test_predictions(ocular_model, test_loader, device)\n",
    "\n",
    "# Calculate final test metrics\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0) \n",
    "\n",
    "print(\"\\n--- Final Test Set Results ---\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(test_labels, test_preds, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Training with 512x512 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: HIGH-RESOLUTION TRAINING (512x512) ---\n",
    "\n",
    "print(\"\\n--- Setting up 512x512 Data Pipeline ---\")\n",
    "\n",
    "# 1. Config\n",
    "# We reduce batch size because 512px images consume much more VRAM\n",
    "# If you get a \"CUDA Out of Memory\" error, reduce this to 8 or 4.\n",
    "HIGH_RES_BATCH_SIZE = 16 \n",
    "\n",
    "# 2. Define High-Res Transforms\n",
    "# We explicitly resize to 512 (or just skip resizing if images are already 512)\n",
    "# to ensure consistency.\n",
    "train_transforms_512 = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "val_test_transforms_512 = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# 3. Re-Initialize Datasets with new transforms\n",
    "train_dataset_512 = OcularDataset(train_df, path, transform=train_transforms_512)\n",
    "val_dataset_512 = OcularDataset(val_df, path, transform=val_test_transforms_512)\n",
    "test_dataset_512 = OcularDataset(test_df, path, transform=val_test_transforms_512)\n",
    "\n",
    "# 4. Re-Initialize Loaders\n",
    "# We reuse the same weighted sampler logic for training\n",
    "train_loader_512 = DataLoader(\n",
    "    train_dataset_512,\n",
    "    batch_size=HIGH_RES_BATCH_SIZE,\n",
    "    sampler=train_sampler, # Reusing the sampler from before\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader_512 = DataLoader(\n",
    "    val_dataset_512,\n",
    "    batch_size=HIGH_RES_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader_512 = DataLoader(\n",
    "    test_dataset_512,\n",
    "    batch_size=HIGH_RES_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"High-Res Loaders ready. Batch Size: {HIGH_RES_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preparing Model for High-Res Fine-Tuning ---\")\n",
    "\n",
    "# 1. Load the best 224px model weights\n",
    "# This is \"Progressive Resizing\": Start with knowledge from small images.\n",
    "ocular_model.load_state_dict(torch.load('best_ocular_cnn_finetuned.pth', weights_only=True))\n",
    "ocular_model.to(device)\n",
    "\n",
    "# 2. Optimizer Setup\n",
    "# We keep the learning rate low to carefully adapt to the new resolution\n",
    "# without destroying previous knowledge.\n",
    "LR_HIGH_RES = 1e-5 \n",
    "optimizer_512 = optim.AdamW(ocular_model.parameters(), lr=LR_HIGH_RES, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"Model loaded. Starting High-Res training with LR={LR_HIGH_RES}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTE HIGH-RES TRAINING ---\n",
    "\n",
    "NUM_EPOCHS_512 = 20 # Start with 10 epochs, it takes longer now!\n",
    "best_val_f1_512 = 0.0\n",
    "\n",
    "# New history lists for this phase\n",
    "history_train_loss_512 = []\n",
    "history_val_loss_512 = []\n",
    "history_val_f1_512 = []\n",
    "history_val_acc_512 = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS_512 + 1):\n",
    "    print(f\"\\nHigh-Res Epoch {epoch}/{NUM_EPOCHS_512}:\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(ocular_model, train_loader_512, criterion, optimizer_512, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = validate_epoch(ocular_model, val_loader_512, criterion, device)\n",
    "\n",
    "    # Store history\n",
    "    history_train_loss_512.append(train_loss)\n",
    "    history_val_loss_512.append(val_loss)\n",
    "    history_val_acc_512.append(val_acc)\n",
    "    history_val_f1_512.append(val_f1)\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} | F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best 512 model\n",
    "    if val_f1 > best_val_f1_512:\n",
    "        best_val_f1_512 = val_f1\n",
    "        torch.save(ocular_model.state_dict(), 'best_ocular_cnn_512.pth')\n",
    "        print(\"  --> High-Res Model saved!\")\n",
    "\n",
    "print(f\"\\n--- High-Res Training Complete. Best F1: {best_val_f1_512:.4f} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Plotting after resizing to 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 4: FINAL EVALUATION (HIGH-RES) ---\n",
    "\n",
    "print(\"\\n--- Running FINAL Evaluation on Test Set (512x512) ---\")\n",
    "\n",
    "# 1. Load the best HIGH-RES model\n",
    "# Wichtig: Wir laden jetzt die .pth Datei, die im High-Res Loop gespeichert wurde\n",
    "ocular_model.load_state_dict(torch.load('best_ocular_cnn_512.pth', weights_only=True))\n",
    "ocular_model.to(device)\n",
    "ocular_model.eval()\n",
    "\n",
    "# 2. Get predictions using the HIGH-RES Test Loader\n",
    "# Wir müssen hier zwingend 'test_loader_512' nutzen, sonst passen die Dimensionen nicht\n",
    "test_preds, test_labels = get_test_predictions(ocular_model, test_loader_512, device)\n",
    "\n",
    "# 3. Calculate Metrics\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n=== ULTIMATE TEST RESULTS ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\n",
    "\n",
    "# 4. Plot Confusion Matrix\n",
    "# Das ist die wichtigste Grafik für euren Bericht!\n",
    "plot_confusion_matrix(test_labels, test_preds, CLASS_NAMES)\n",
    "\n",
    "# 5. Optional: Plot High-Res History\n",
    "# Damit man sieht, wie sich das Modell in den letzten 20 Epochen stabilisiert hat\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_train_loss_512, 'bo-', label='Train Loss')\n",
    "plt.plot(history_val_loss_512, 'ro-', label='Val Loss')\n",
    "plt.title('High-Res Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_val_f1_512, 'go-', label='Val F1')\n",
    "plt.plot(history_val_acc_512, 'yo--', label='Val Accuracy')\n",
    "plt.title('High-Res Validation Metrics')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
