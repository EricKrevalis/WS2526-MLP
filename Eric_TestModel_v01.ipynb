{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# First setup to test our data\n",
    "\n",
    "- create train-test-split (70-15-15)\n",
    "- set up a model to train\n",
    "- observe and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! is used to run console commands in jupyter notebooks\n",
    "!pip install -q nbstripout\n",
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './KaggleCache/datasets/andrewmvd/ocular-disease-recognition-odir5k/versions/2'\n",
    "df = pd.read_csv(os.path.join(path, 'full_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND DATA ASSUMPTION ---\n",
    "\n",
    "# Define the split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "# The number of unique classes\n",
    "NUM_CLASSES = 8\n",
    "\n",
    "# --- 2. DATA PREPROCESSING: CONVERT TARGET TO CLASS INDEX ---\n",
    "\n",
    "def target_string_to_index(target_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Converts a string representation of a one-hot list (e.g., '[0,1,0,...]')\n",
    "    into a single integer class index (e.g., 1).\n",
    "    \"\"\"\n",
    "    # Use ast.literal_eval for safe string-to-list conversion\n",
    "    target_list = ast.literal_eval(target_str)\n",
    "    # The index of '1' is the class index\n",
    "    return target_list.index(1)\n",
    "\n",
    "# Apply the conversion to create the necessary column for stratification\n",
    "df['class_index'] = df['target'].apply(target_string_to_index)\n",
    "\n",
    "# Print initial class distribution\n",
    "print(\"--- Initial Class Distribution ---\")\n",
    "print(df['class_index'].value_counts().sort_index())\n",
    "print(\"-\" * 34)\n",
    "\n",
    "# --- 3. STRATIFIED TRAIN / TEST / VAL SPLIT (70/15/15) ---\n",
    "\n",
    "# Step 1: Split into Training (70%) and Temporary (30%) sets\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=(VAL_RATIO + TEST_RATIO), # 0.15 + 0.15 = 0.30\n",
    "    stratify=df['class_index'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split Temporary (30%) into Validation (15%) and Test (15%) sets\n",
    "# test_size = 0.5 because 0.5 of the remaining 0.30 is 0.15\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['class_index'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Verify the final split sizes\n",
    "print(\"\\n--- Final Dataset Sizes ---\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Training Samples (70%): {len(train_df)}\")\n",
    "print(f\"Validation Samples (15%): {len(val_df)}\")\n",
    "print(f\"Test Samples (15%): {len(test_df)}\")\n",
    "\n",
    "# --- 4. CALCULATE INVERSE CLASS FREQUENCY FOR WEIGHTED SAMPLER ---\n",
    "\n",
    "# Count occurrences of each class in the training set\n",
    "class_counts = Counter(train_df['class_index'])\n",
    "# Get total number of samples in the training set\n",
    "total_samples = len(train_df)\n",
    "# Calculate the frequency of each class\n",
    "class_frequencies = {i: class_counts.get(i, 0) / total_samples for i in range(NUM_CLASSES)}\n",
    "\n",
    "# Calculate inverse frequency (or weight)\n",
    "# The weight for a class is inversely proportional to its frequency: w_i = 1 / f_i\n",
    "# We use this as the basis for the PyTorch WeightedRandomSampler\n",
    "class_weights = {\n",
    "    i: 1.0 / class_frequencies[i]\n",
    "    for i in range(NUM_CLASSES) if class_frequencies[i] > 0\n",
    "}\n",
    "\n",
    "# Convert weights to a tensor (PyTorch requires this format)\n",
    "# Note: PyTorch expects weights ordered by class index [w0, w1, w2, ...]\n",
    "# Use max(class_weights.values()) for normalization, but absolute inverse frequency is fine too\n",
    "inverse_weights = [class_weights.get(i, 0.0) for i in range(NUM_CLASSES)]\n",
    "# Normalize the weights so the smallest weight is 1.0\n",
    "max_weight = max(inverse_weights)\n",
    "normalized_weights = [w / max_weight for w in inverse_weights]\n",
    "\n",
    "# Print the final class weights for review\n",
    "print(\"\\n--- Training Set Class Weights (Normalized) ---\")\n",
    "print(f\"Class Frequencies: {class_frequencies}\")\n",
    "print(f\"Inverse Weights: {normalized_weights}\")\n",
    "# Store the weights as a numpy array for easy conversion to PyTorch tensor later\n",
    "class_weights_np = np.array(normalized_weights, dtype=np.float32)\n",
    "\n",
    "print(\"\\n--- Stratification Check (Training Set) ---\")\n",
    "print(train_df['class_index'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"\\n--- Stratification Check (Validation Set) ---\")\n",
    "print(val_df['class_index'].value_counts(normalize=True).sort_index() * 100)\n",
    "print(\"\\n--- Stratification Check (Test Set) ---\")\n",
    "print(test_df['class_index'].value_counts(normalize=True).sort_index() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Example CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. MODEL ARCHITECTURE DEFINITION (REVISED) ---\n",
    "\n",
    "class OcularCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom Convolutional Neural Network (CNN) designed for 512x512 ocular images.\n",
    "    \n",
    "    REVISION: Implemented Global Average Pooling (GAP) to drastically reduce\n",
    "    the parameter count from ~67M to under 1M, mitigating massive overfitting.\n",
    "    The network is also deepened with a fourth convolutional block.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=8):\n",
    "        \"\"\"\n",
    "        Defines all the layers required for the CNN.\n",
    "        \"\"\"\n",
    "        super(OcularCNN, self).__init__()\n",
    "        \n",
    "        # --- Feature Extractor (4 Convolutional Blocks) ---\n",
    "        \n",
    "        # Block 1: Input 3x512x512 -> Output 16x128x128\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        # MaxPool with large stride (4) to rapidly reduce spatial size (512/4 = 128)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4) \n",
    "        \n",
    "        # Block 2: Input 16x128x128 -> Output 32x64x64\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # MaxPool reduces size by half (128/2 = 64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        \n",
    "        # Block 3: Input 32x64x64 -> Output 64x32x32\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        # MaxPool reduces size by half (64/2 = 32)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        \n",
    "        # New Block 4: Input 64x32x32 -> Output 128x16x16 (Deeper feature extraction)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        # MaxPool reduces size by half (32/2 = 16)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # --- Parameter Reduction and Classifier ---\n",
    "        \n",
    "        # KEY FIX: AdaptiveAvgPool2d (Global Average Pooling)\n",
    "        # Reduces 128x16x16 feature map to 128x1x1 (vector of 128 features)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5) \n",
    "        \n",
    "        # Final fully connected layer now only takes 128 features as input\n",
    "        self.fc128 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        self.fc64 = nn.Linear(in_features=64, out_features=num_classes)\n",
    "        self.fc65536 = nn.Linear(in_features=65536, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the explicit flow of data through the network.\n",
    "        \"\"\"\n",
    "        # Block 1\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        # Block 2\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        # Block 3\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Block 4\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global Average Pooling (128x16x16 -> 128x1x1)\n",
    "        x = self.gap(x)\n",
    "        \n",
    "        # Flatten the 128x1x1 tensor into a 128-element vector\n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        # Classifier\n",
    "        x = self.dropout(x)\n",
    "        # Output layer (no activation, outputs logits)\n",
    "        x = self.fc128(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# --- 2. MODEL PARAMETER ANALYSIS UTILITY ---\n",
    "\n",
    "def display_model_summary(model, input_size=(3, 512, 512)):\n",
    "    \"\"\"\n",
    "    Analyzes and prints the layer-by-layer details of the network.\n",
    "    \n",
    "    This is a critical step to verify the feature dimensions (especially before\n",
    "    the first fully connected layer) and total parameter count.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model instance.\n",
    "        input_size (tuple): The expected C x H x W input size.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Model Summary and Parameter Analysis ---\")\n",
    "    \n",
    "    # Check if torchsummary is available (standard in industry, but optional dependency)\n",
    "    try:\n",
    "        from torchsummary import summary\n",
    "        # Summary utility requires the model on CPU and uses a single batch size (1)\n",
    "        summary(model.to('cpu'), input_size=input_size)\n",
    "    except ImportError:\n",
    "        # Manual parameter counting fallback (Pure PyTorch approach)\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"Note: 'torchsummary' not found. Displaying manual parameter count:\")\n",
    "        print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "        \n",
    "        # Test a forward pass to check final shape before FC layer\n",
    "        test_input = torch.randn(1, *input_size)\n",
    "        try:\n",
    "            output = model.pool4(F.relu(model.bn4(model.conv4(test_input))))\n",
    "            output = model.gap(output)\n",
    "            final_features = output.view(output.size(0), -1).size(1)\n",
    "            print(f\"Feature Vector Size before FC layer: {final_features:,}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error during feature check: {e}\")\n",
    "\n",
    "# --- 3. MODEL INSTANTIATION AND ANALYSIS ---\n",
    "\n",
    "# Instantiate the custom model\n",
    "ocular_model = OcularCNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Move the model to the GPU device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ocular_model.to(device)\n",
    "\n",
    "# Display the model architecture and parameter count\n",
    "# Note: NUM_CLASSES is assumed to be defined from the previous cell's execution\n",
    "display_model_summary(ocular_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Batch size is a critical hyperparameter; 32 is a common starting point for 512x512\n",
    "BATCH_SIZE = 32\n",
    "# Assumed to be available from previous cells:\n",
    "# train_df, val_df, test_df (DataFrames for splits)\n",
    "# path (Root path string)\n",
    "# class_weights_np (Numpy array of class weights for WeightedRandomSampler)\n",
    "\n",
    "# --- 2. CUSTOM DATASET CLASS (The PyTorch Way) ---\n",
    "\n",
    "class OcularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading ocular fundus images and labels.\n",
    "    Handles image loading, preprocessing, and string-to-index target parsing.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing 'filename' and 'class_index'.\n",
    "            root_dir: Base directory containing the 'preprocessed_images' folder.\n",
    "            transform: Composed torchvision transforms to apply to the image.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(self.root_dir, \"preprocessed_images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads one sample (image and label) based on index.\n",
    "        \"\"\"\n",
    "        # Get filename and class index from the DataFrame\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['filename']\n",
    "        # Target is an integer class index (0-7), converted to LongTensor for CrossEntropyLoss\n",
    "        label = row['class_index'] \n",
    "        \n",
    "        # Construct the full image path\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load the image using PIL (standard for torchvision transforms)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the tensor and the label\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- 3. TRANSFORMS DEFINITION ---\n",
    "\n",
    "# Define normalization parameters (standard for ImageNet-trained models, good baseline)\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training Transforms (includes augmentations)\n",
    "# Note: Resize and CenterCrop are omitted as images are already 512x512\n",
    "train_transforms = transforms.Compose([\n",
    "    # Augmentations for robustness and generalization\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    # Convert image (HWC) to PyTorch Tensor (CHW) and normalize pixel values\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# Validation/Test Transforms (deterministic)\n",
    "# Note: OnlyToTensor and Normalize are applied for consistent evaluation\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD)\n",
    "])\n",
    "\n",
    "# --- 4. DATASET INSTANTIATION ---\n",
    "\n",
    "train_dataset = OcularDataset(train_df, path, transform=train_transforms)\n",
    "val_dataset = OcularDataset(val_df, path, transform=val_test_transforms)\n",
    "test_dataset = OcularDataset(test_df, path, transform=val_test_transforms)\n",
    "\n",
    "# --- 5. DATA LOADER CONFIGURATION (Addressing Imbalance) ---\n",
    "\n",
    "# Calculate sample weights for the Training DataLoader\n",
    "# Map the class index of every sample in the training set to its inverse weight\n",
    "sample_weights = train_df['class_index'].apply(lambda x: class_weights_np[x]).values\n",
    "# Create the sampler: replaces the standard shuffling behavior\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights), # Sample size equals the full dataset length\n",
    "    replacement=True # Must be True for random selection with replacement\n",
    ")\n",
    "\n",
    "# Instantiate the DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler, # Use the custom sampler instead of 'shuffle=True'\n",
    "    num_workers=4, # Use multiple threads for faster data loading (best practice)\n",
    "    pin_memory=True # Speeds up data transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No shuffling needed for validation\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No shuffling needed for testing\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- 6. SANITY CHECK ---\n",
    "\n",
    "print(f\"\\n--- DataLoader Sanity Check ---\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check one batch to verify tensor shapes and device readiness\n",
    "for images, labels in train_loader:\n",
    "    print(f\"\\nSample Batch Test (Training Loader):\")\n",
    "    print(f\"Image Tensor Shape (B x C x H x W): {images.shape}\")\n",
    "    print(f\"Label Tensor Shape (B): {labels.shape}\")\n",
    "    print(f\"Label Data Type (Should be torch.long): {labels.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Hyperparameters (initial estimates)\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "# REGULARIZATION FIXES for Overfitting and high complexity\n",
    "# PATIENCE removed to disable Early Stopping, allowing full run\n",
    "WEIGHT_DECAY = 1e-5    # L2 regularization to penalize large weights and reduce overfitting\n",
    "\n",
    "# Assumed to be available from previous cells:\n",
    "# ocular_model (Instance of OcularCNN, moved to 'device')\n",
    "# train_loader, val_loader (DataLoaders)\n",
    "# device (torch.device('cuda:0' or 'cpu'))\n",
    "# NUM_CLASSES (int, 8)\n",
    "\n",
    "# --- 2. INITIALIZATION ---\n",
    "\n",
    "# Define the optimizer (AdamW is an industry standard upgrade over Adam)\n",
    "# FIX: Added weight_decay for L2 Regularization to combat large parameter count overfitting\n",
    "optimizer = optim.AdamW(ocular_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define the standard loss function (CrossEntropyLoss)\n",
    "# Note: This loss expects raw logits (no softmax) and integer class indices (0-7)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# CRITICAL FIX: Ensure the loss function is also on the GPU device\n",
    "criterion.to(device) \n",
    "\n",
    "# --- 3. TRAINING FUNCTION DEFINITION ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Executes a single training epoch with explicit forward and backward passes.\n",
    "    \"\"\"\n",
    "    model.train() # Set the model to training mode (enables dropout/batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap the loader with tqdm for a progress bar\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        # 1. Device Transfer: Move data to the active device (GPU)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 2. Optimization Step 1: Zero the gradients\n",
    "        # Crucial in PyTorch to prevent gradient accumulation from previous batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 3. Forward Pass: Compute model output (logits)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 4. Loss Calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 5. Optimization Step 2: Backward Pass (Backpropagation)\n",
    "        # Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # 6. Optimization Step 3: Update Weights\n",
    "        # Optimizer steps, adjusting parameters based on calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# --- 4. VALIDATION FUNCTION DEFINITION ---\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set without updating weights.\n",
    "    Also collects raw predictions and labels for confusion matrices.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode (disables dropout/batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient calculations during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            # Device Transfer\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Convert logits to predicted class indices\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and true labels for metric calculation on CPU\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # F1-score is important for imbalanced data; 'weighted' accounts for imbalance\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0) \n",
    "    \n",
    "    # Return metrics PLUS raw arrays needed for confusion matrix\n",
    "    return epoch_loss, accuracy, f1, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# --- 5. MAIN TRAINING LOOP EXECUTION ---\n",
    "\n",
    "print(f\"\\n--- Starting Training on {device} for {NUM_EPOCHS} epochs ---\")\n",
    "\n",
    "# ARCHITECT FIX: Re-assert model device to fix the RuntimeError \n",
    "# This ensures all model parameters are definitely on the GPU before training starts.\n",
    "ocular_model.to(device)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "# Initialize lists to store metrics history for later plotting\n",
    "history_train_loss = []\n",
    "history_val_loss = []\n",
    "history_val_acc = []\n",
    "history_val_f1 = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(ocular_model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate (Note: validation now returns raw predictions and labels too)\n",
    "    val_loss, val_acc, val_f1, _, _ = validate_epoch(ocular_model, val_loader, criterion, device)\n",
    "\n",
    "    # Store history\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_val_loss.append(val_loss)\n",
    "    history_val_acc.append(val_acc)\n",
    "    history_val_f1.append(val_f1)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} | F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Model Checkpointing Logic (Save only if improved)\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        # Save only the model's learned parameters (state_dict)\n",
    "        torch.save(ocular_model.state_dict(), 'best_ocular_cnn.pth')\n",
    "        print(\"  --> Model saved! New best F1-Score achieved.\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation F1-Score: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND ASSUMPTIONS ---\n",
    "\n",
    "# Assumed to be available from previous cells:\n",
    "# ocular_model (Instance of OcularCNN)\n",
    "# test_loader (DataLoader for the final, unseen data)\n",
    "# device (torch.device('cuda:0' or 'cpu'))\n",
    "# history_train_loss, history_val_loss, history_val_f1, history_val_acc (Metric lists from training)\n",
    "# NUM_CLASSES (int, 8)\n",
    "\n",
    "# Define class names for the confusion matrix display\n",
    "# The order must match the class indices (0 to 7)\n",
    "CLASS_NAMES = [\n",
    "    \"Normal\", \"Diabetic\", \"Glaucoma\", \"Cataract\", \n",
    "    \"Macular Deg.\", \"Retinal Detach.\", \"Hypertensive\", \"Other\"\n",
    "]\n",
    "\n",
    "# --- 2. PLOTTING FUNCTIONS ---\n",
    "\n",
    "def plot_training_metrics(train_loss, val_loss, val_f1, val_acc):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss and F1-score/Accuracy over epochs.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Loss Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (CrossEntropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot 2: F1 and Accuracy Scores\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_f1, 'go-', label='Validation F1-Score (Weighted)')\n",
    "    plt.plot(epochs, val_acc, 'yo--', label='Validation Accuracy')\n",
    "    plt.title('Validation Performance')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_test_predictions(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Runs the model on the test set to collect all predictions and true labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "            # Get the predicted class index\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, class_names):\n",
    "    \"\"\"\n",
    "    Generates and plots the confusion matrix for final classification results.\n",
    "    \"\"\"\n",
    "    # Calculate the raw confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=np.arange(len(class_names)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Use heatmap for visualization with seaborn\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', # 'd' formats the numbers as integers\n",
    "        cmap='Blues', \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "# --- 3. EXECUTION ---\n",
    "\n",
    "# 1. Plot Training History\n",
    "print(\"\\n--- Plotting Training Metrics ---\")\n",
    "plot_training_metrics(\n",
    "    history_train_loss, \n",
    "    history_val_loss, \n",
    "    history_val_f1, \n",
    "    history_val_acc\n",
    ")\n",
    "\n",
    "# 2. Evaluate on Test Set and Plot Confusion Matrix\n",
    "\n",
    "# Load the best weights saved during training\n",
    "# FIX: Added weights_only=True to comply with PyTorch best practice and remove Future Warning\n",
    "ocular_model.load_state_dict(torch.load('best_ocular_cnn.pth', weights_only=True))\n",
    "ocular_model.to(device) # Ensure model is on device before running inference\n",
    "\n",
    "# Get predictions and true labels from the unseen test set\n",
    "test_preds, test_labels = get_test_predictions(ocular_model, test_loader, device)\n",
    "\n",
    "# Calculate final test metrics\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0) \n",
    "\n",
    "print(\"\\n--- Final Test Set Results ---\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(test_labels, test_preds, CLASS_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
